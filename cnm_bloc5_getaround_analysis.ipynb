{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc 5 - Industrialisation d'un algorithme d'apprentissage automatique et automatisation des processus de d√©cision - Getaround analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Getaround is an American online car sharing service founded in 2009 in San Francisco and launched to the public in 2011. It is now the European leader for car sharing, connecting drivers who need a car to car owners.\n",
    "\n",
    "### Problematic\n",
    "\n",
    "- 1 - Getaround has experienced problems with unsatisfied users due to late car returns from the previous driver. The company would like to optimize the delay between two rentals.\n",
    "\n",
    "- 2 - Getaround currently works on pricing optimization. The company would like to suggest an optimum price to car owners depending on the characteristics of their car.\n",
    "\n",
    "### Scope\n",
    "\n",
    "To optimize the delay between two rentals, the product manager of Getaround needs some insights to define the right trade off that would solve the late check-outs issue without impacting the car owners' revenues. For this purpose, Getaround provided a dataset containing information on car shares, which included information on potential cancelation of the shares, check-out time for ended shares, and information on the previous share when applicable.\n",
    "\n",
    "To optimize the price of rentals for car owners, Getaround would like to develop an API that would predict an optimum price based on cars' characteristics. For this purpose, the data science team provided a dataset containing information about cars, and the corresponding prices that are currently applied.\n",
    "\n",
    "### Aims and objectives\n",
    "\n",
    "**Aim 1: Build a dashboard that would help the product manager make a decision about late check-outs.**\n",
    "\n",
    "Objectives:\n",
    "- 1 - Evaluate the proportion of late check-outs and their impact on the next driver.\n",
    "- 2 - Evaluate the benefits of the new feature for car drivers and car owners.\n",
    "- 3 - Evaluate the global impact of the new feature on car rentals and cancelations.\n",
    "- 4 - Evaluate the proportion of problematic cases solved by the new feature and the proportion of rentals that would be affected.\n",
    "\n",
    "**Aim 2: Build an API that would predict an optimum rental price depending on cars' features.**\n",
    "\n",
    "Objectives:\n",
    "- 1 - Develop a machine learning model that would predict an optimum rental price.\n",
    "- 2 - Develop an API for price prediction.\n",
    "- 3 - Test the API functionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Methods\n",
    "\n",
    "### 1 - Library import\n",
    "\n",
    "### 2 - File reading and basic exploration\n",
    "\n",
    "The dataset related to the analysis of late check-outs was composed of 7 features describing 21.310 car shares. It contained missing values for some crucial information about previous shares and previous delays at check-out.\n",
    "\n",
    "The dataset related to price prediction described 4.843 cars' characteristics, among which the brand, the mileage, the power engine, records on many options, and the rental price per day. It did not contain any missing value.\n",
    "\n",
    "### 3 - Analysis of delay data\n",
    "\n",
    "It was noticeable that more than 90% of the provided data did not contain any information about previous shares. The decision was made to keep this data for the current analysis, following the assumption that these shares might correspond to episodic shares, new users of the service, or possibly unsatisfied users who share their car only once (45% of the dataset). \n",
    "\n",
    "To ease the use of the dataset, features were renamed. The data was then augmented with the delay of the previous driver. Shares that did not have information on the delay at check-out (for ended rentals) were dropped from the data. Shares that did show a previous driver but did not have information on the previous delay were also discarded for the analysis.\n",
    "\n",
    "Then, to build the dashboard, data was obtained for each figure and saved to later be plotted. Results of the analysis were published online at https://cnmgetaroundanalysis.herokuapp.com.\n",
    "\n",
    "### 4 - Prediction of pricing\n",
    "\n",
    "After removal of outliers, data was preprocessed for machine learning. A simple linear regression model was trained. The R2 scores on train and test sets are 0.69 and 0.71, respectively. The preprocessor and the model were saved to be used in the API. The API, published online at https://cnmgetaroundprediction.herokuapp.com/ was tested after its development."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Conclusion\n",
    "\n",
    "Regarding the analysis of late check-outs, a dashboard is available online. It shows that a delay of 1 hour between two rentals would solve 50% of problematic cases if applied to all cars, while only affecting 1.7% of all rentals.\n",
    "\n",
    "Regarding the price prediction, an API is also available online to optimize the rental price of cars depending on their features. The machine learning model could be further optimized in future developments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 - library import ### ----\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - File reading and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - import dataset ### ----\n",
    "\n",
    "# load data\n",
    "data = pd.read_excel(\"cnm_bloc5_get_around_delay_analysis.xlsx\")\n",
    "data_ml = pd.read_csv(\"cnm_bloc5_get_around_pricing_project.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats on data ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Data: Delays\")\n",
    "print()\n",
    "print(\"Number of rows: {}\".format(data.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "# display percentage of missing values in columns and rows\n",
    "percent_nan_col = data.isnull().sum() / data.shape[0] * 100\n",
    "print(\"Percentage of missing values per column:\\n{}\".format(percent_nan_col))\n",
    "print()\n",
    "percent_nan_row = data[data.isnull().all(axis = 1)].shape[0] / data.shape[1] * 100\n",
    "print(\"Percentage of rows fully filled with missing values: {}\".format(percent_nan_row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats on data_ml ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Data: Prices\")\n",
    "print()\n",
    "print(\"Number of rows: {}\".format(data_ml.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data_ml.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data_ml.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data_ml.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "# display percentage of missing values in columns and rows\n",
    "percent_nan_col = data_ml.isnull().sum() / data_ml.shape[0] * 100\n",
    "print(\"Percentage of missing values per column:\\n{}\".format(percent_nan_col))\n",
    "print()\n",
    "percent_nan_row = data_ml[data_ml.isnull().all(axis = 1)].shape[0] / data_ml.shape[1] * 100\n",
    "print(\"Percentage of rows fully filled with missing values: {}\".format(percent_nan_row))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - Analysis of delay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 0 ### ----\n",
    "\n",
    "# number of shares per car owner\n",
    "\n",
    "# get unique cars\n",
    "cars_unique = data[\"car_id\"].unique()\n",
    "\n",
    "# initialise dataframe to store data\n",
    "data_plot0 = pd.DataFrame(cars_unique, columns = [\"car_id\"])\n",
    "\n",
    "# fill data per car owner\n",
    "data_plot0[\"count_total\"] = [data.loc[data[\"car_id\"] == car,:].shape[0] for car in data_plot0[\"car_id\"]]\n",
    "\n",
    "# get percent of cars with only one share\n",
    "percent1car = data_plot0.loc[data_plot0[\"count_total\"] == 1,:].shape[0] / data_plot0.shape[0] * 100\n",
    "\n",
    "# get cause for sharing car only once\n",
    "cars_one_share = data_plot0.loc[data_plot0[\"count_total\"] == 1,\"car_id\"]\n",
    "data_one_share = data.loc[data[\"car_id\"].isin(cars_one_share),:]\n",
    "percent_canceled = data_one_share.loc[data_one_share[\"state\"] == \"canceled\",:].shape[0] / \\\n",
    "    data_one_share.shape[0] * 100\n",
    "\n",
    "# report\n",
    "print(\"Percentage of owners that rent their car only once: {}%\".format(np.round(percent1car,1)))\n",
    "print(\"Percentage of cancelations for owners that rent their car only once: {}%\".format(np.round(percent_canceled,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - preprocess data ### ----\n",
    "\n",
    "# rename columns \n",
    "data.rename(columns = {\"delay_at_checkout_in_minutes\": \"delay\", \"previous_ended_rental_id\": \"previous_id\",\n",
    "    \"time_delta_with_previous_rental_in_minutes\": \"time_delta\"}, inplace = True)\n",
    "\n",
    "# augment data with delay from previous rental\n",
    "for i in range(0,data.shape[0]):\n",
    "    if np.isnan(data.loc[i,\"previous_id\"]):\n",
    "        data.loc[i,\"previous_delay\"] = np.NaN\n",
    "    else:\n",
    "        previous_rental = data.loc[i,\"previous_id\"]\n",
    "        data.loc[i,\"previous_delay\"] = data.loc[data[\"rental_id\"] == previous_rental,\"delay\"].values[0]\n",
    "\n",
    "# drop rows that contain a missing value for check-out delay (only for ended rentals)\n",
    "# (these rows could not be analysed and would mess up analysis)\n",
    "index_drop = data.loc[(data[\"delay\"].isnull()) & (data[\"state\"] == \"ended\"),:].index\n",
    "data = data.drop(index_drop, axis = 0).reset_index(drop = True)\n",
    "\n",
    "# drop rows that contain a missing value for previous delay (only if previous_id is known)\n",
    "# (these rows could not be analysed and would mess up analysis)\n",
    "index_drop = data.loc[(data[\"previous_delay\"].isnull()) & (data[\"previous_id\"].notnull()),:].index\n",
    "data = data.drop(index_drop, axis = 0).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 1 ### ----\n",
    "\n",
    "# proportion on late vs on time check-outs\n",
    "\n",
    "# get data\n",
    "data_plot1 = data.loc[data[\"state\"] == \"ended\",[\"state\",\"delay\"]]\n",
    "\n",
    "# add column with encoded delay\n",
    "data_plot1[\"checkout\"] = data_plot1[\"delay\"].apply(lambda x: \"on time\" if x <= 0 else \"late\")\n",
    "\n",
    "# rename and save\n",
    "data_plot1.to_csv(\"streamlit/cnm_bloc5_data_plot1.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 2 ### ----\n",
    "\n",
    "# distribution of delays\n",
    "\n",
    "# get data\n",
    "data_plot2 = data_plot1.loc[data_plot1[\"delay\"] > 0,:]\n",
    "\n",
    "# drop outliers for plotting\n",
    "upper_bond = data_plot2[\"delay\"].mean() + 2 * data_plot2[\"delay\"].std()\n",
    "data_plot2 = data_plot2.loc[data_plot2[\"delay\"] < upper_bond,:]\n",
    "\n",
    "# rename and save\n",
    "data_plot2.to_csv(\"streamlit/cnm_bloc5_data_plot2.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 3 ### ----\n",
    "\n",
    "# proportion of cancelations after delayed check-outs\n",
    "\n",
    "# get rental id of late check-outs\n",
    "mask_late = (data[\"state\"] == \"ended\") & (data[\"delay\"] > 0)\n",
    "late_checkouts = data.loc[mask_late,\"rental_id\"]\n",
    "\n",
    "# get data\n",
    "data_plot3 = data.loc[data[\"previous_id\"].isin(late_checkouts),:]\n",
    "\n",
    "# rename and save\n",
    "data_plot3.to_csv(\"streamlit/cnm_bloc5_data_plot3.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 4 ### ----\n",
    "\n",
    "# proportion of car drivers that would benefit from the new feature\n",
    "\n",
    "# get data\n",
    "data_plot4 = data.loc[:,[\"previous_id\",\"previous_delay\"]]\n",
    "\n",
    "# get benefit per rental\n",
    "data_plot4[\"benefit\"] = \"no benefit\"\n",
    "data_plot4.loc[(data_plot4[\"previous_id\"].notnull()) & (data_plot4[\"previous_delay\"] > 0),\"benefit\"] = \"benefit\"\n",
    "\n",
    "# rename and save\n",
    "data_plot4.to_csv(\"streamlit/cnm_bloc5_data_plot4.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 5 ### ----\n",
    "\n",
    "# proportion of car owners that would benefit from the new feature\n",
    "\n",
    "# get unique cars\n",
    "cars_unique = data[\"car_id\"].unique()\n",
    "\n",
    "# initialise dataframe to store data\n",
    "data_plot5 = pd.DataFrame(cars_unique, columns = [\"car_id\"])\n",
    "\n",
    "# fill data per car owner\n",
    "for i in range(0,data_plot5.shape[0]):\n",
    "    # set masks\n",
    "    mask_car = data[\"car_id\"] == data_plot5.loc[i,\"car_id\"]\n",
    "    mask_canceled = data[\"state\"] == \"canceled\"\n",
    "    mask_delay = data[\"previous_delay\"] > 0\n",
    "    # fill data\n",
    "    data_plot5.loc[i,\"count_total\"] = data.loc[mask_car,:].shape[0]\n",
    "    data_plot5.loc[i,\"count_canceled_delay\"] = data.loc[mask_car & mask_canceled & mask_delay,:].shape[0]\n",
    "\n",
    "# get benefit per car owner\n",
    "data_plot5[\"benefit\"] = data_plot5[\"count_canceled_delay\"].apply(lambda x: \"benefit\" if x > 0 else \"no benefit\")\n",
    "\n",
    "# rename and save\n",
    "data_plot5.to_csv(\"streamlit/cnm_bloc5_data_plot5.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 6 ### ----\n",
    "\n",
    "# proportion of canceled rentals due to delay per car owner\n",
    "\n",
    "# re-use data from previous plot\n",
    "data_plot6 = data_plot5.copy()\n",
    "\n",
    "# get percentage of cancelations due to previous delay for each car owner\n",
    "data_plot6[\"percent_canceled_delay\"] = data_plot6[\"count_canceled_delay\"] / data_plot6[\"count_total\"] * 100\n",
    "\n",
    "# keep only beneficiaries\n",
    "data_plot6 = data_plot6.loc[data_plot6[\"count_canceled_delay\"] > 0,:]\n",
    "\n",
    "# rename and save\n",
    "data_plot6.to_csv(\"streamlit/cnm_bloc5_data_plot6.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 7 ### ----\n",
    "\n",
    "# proportion of cancelations due to previous delay\n",
    "\n",
    "# get data\n",
    "data_plot7 = data.loc[data[\"state\"] == \"canceled\",[\"state\",\"previous_delay\"]]\n",
    "\n",
    "# add column with encoded cause for cancelation\n",
    "data_plot7[\"cause\"] = data[\"previous_delay\"].apply(lambda x: \"not known\" if np.isnan(x) else \"delay\" if x > 0 \n",
    "    else \"no delay\")\n",
    "\n",
    "# rename and save\n",
    "data_plot7.to_csv(\"streamlit/cnm_bloc5_data_plot7.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 8 ### ----\n",
    "\n",
    "# proportion of rentals affected by the new feature\n",
    "\n",
    "# set thresholds\n",
    "thresholds = np.arange(0,450,30)\n",
    "\n",
    "# initialise variable to store results\n",
    "data_plot81 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "data_plot82 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "\n",
    "# get percentage of rentals affected depending threshold (all cars)\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot81.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot81.loc[i,\"type\"] = \"all cars\"\n",
    "    data_plot81.loc[i,\"percent_total\"] = data.loc[data[\"previous_delay\"] > thresholds[i],:].shape[0] / \\\n",
    "        data.shape[0] * 100\n",
    "    \n",
    "# get percentage of rentals affected depending threshold (connect cars)\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot82.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot82.loc[i,\"type\"] = \"connect cars\"\n",
    "    data_plot82.loc[i,\"percent_total\"] = data.loc[(data[\"checkin_type\"] == \"connect\") & \n",
    "        (data[\"previous_delay\"] > thresholds[i]),:].shape[0] / data.shape[0] * 100\n",
    "    \n",
    "# compile data\n",
    "data_plot8 = pd.concat([data_plot81,data_plot82], axis = 0)\n",
    "\n",
    "# rename and save\n",
    "data_plot8.to_csv(\"streamlit/cnm_bloc5_data_plot8.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 9 ### ----\n",
    "\n",
    "# proportion of cancelations concerned by the new feature\n",
    "\n",
    "# set thresholds\n",
    "thresholds = np.arange(0,450,30)\n",
    "\n",
    "# initialise variable to store results\n",
    "data_plot91 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "data_plot92 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "\n",
    "# get percentage of cancelations affected depending threshold (all cars)\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot91.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot91.loc[i,\"type\"] = \"all cars\"\n",
    "    data_plot91.loc[i,\"percent_total\"] = data.loc[(data[\"state\"] == \"canceled\") & \\\n",
    "        (data[\"previous_delay\"] > thresholds[i]),:].shape[0] / \\\n",
    "        data.loc[data[\"state\"] == \"canceled\",:].shape[0] * 100\n",
    "    \n",
    "# get percentage of cancelations affected depending threshold (connect cars)\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot92.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot92.loc[i,\"type\"] = \"connect cars\"\n",
    "    data_plot92.loc[i,\"percent_total\"] = data.loc[(data[\"state\"] == \"canceled\") & \\\n",
    "        (data[\"checkin_type\"] == \"connect\") & (data[\"previous_delay\"] > thresholds[i]),:].shape[0] / \\\n",
    "        data.loc[data[\"state\"] == \"canceled\",:].shape[0] * 100\n",
    "    \n",
    "# compile data\n",
    "data_plot9 = pd.concat([data_plot91,data_plot92], axis = 0)\n",
    "\n",
    "# rename and save\n",
    "data_plot9.to_csv(\"streamlit/cnm_bloc5_data_plot9.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 10 ### ----\n",
    "\n",
    "# proportion of cancelations due to delay concerned by the new feature\n",
    "\n",
    "# set thresholds\n",
    "thresholds = np.arange(0,450,30)\n",
    "\n",
    "# initialise variable to store results\n",
    "data_plot101 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "data_plot102 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "\n",
    "# get percentage of cancelations fixed depending threshold (all cars)\n",
    "mask = (data[\"state\"] == \"canceled\") & (data[\"previous_delay\"] > 0)\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot101.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot101.loc[i,\"type\"] = \"all cars\"\n",
    "    data_plot101.loc[i,\"percent_total\"] = data.loc[mask & (data[\"previous_delay\"] < thresholds[i]),:].shape[0] / \\\n",
    "        data.loc[mask,:].shape[0] * 100\n",
    "    \n",
    "# get percentage of cancelations fixed depending threshold (connect cars)\n",
    "mask = (data[\"state\"] == \"canceled\") & (data[\"previous_delay\"] > 0)\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot102.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot102.loc[i,\"type\"] = \"connect cars\"\n",
    "    data_plot102.loc[i,\"percent_total\"] = data.loc[mask & (data[\"checkin_type\"] == \"connect\") & \\\n",
    "        (data[\"previous_delay\"] < thresholds[i]),:].shape[0] / data.loc[mask,:].shape[0] * 100\n",
    "    \n",
    "# compile data\n",
    "data_plot10 = pd.concat([data_plot101,data_plot102], axis = 0)\n",
    "\n",
    "# rename and save\n",
    "data_plot10.to_csv(\"streamlit/cnm_bloc5_data_plot10.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - analysis of delay data  - get and save data for plot 11 ### ----\n",
    "\n",
    "# proportion of car shares that would be negatively affected by the new feature\n",
    "\n",
    "# set thresholds\n",
    "thresholds = np.arange(0,450,30)\n",
    "\n",
    "# initialise variable to store results\n",
    "data_plot111 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "data_plot112 = pd.DataFrame(index = range(0,len(thresholds)), columns = [\"threshold\",\"type\",\"percent_total\"])\n",
    "\n",
    "# get percentage of car shares affected depending threshold (all cars)\n",
    "mask = (data[\"state\"] == \"ended\") & (data[\"previous_id\"].notnull())\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot111.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot111.loc[i,\"type\"] = \"all cars\"\n",
    "    data_plot111.loc[i,\"percent_total\"] = data.loc[mask & (data[\"time_delta\"] < thresholds[i]),:].shape[0] / \\\n",
    "        data.shape[0] * 100\n",
    "    \n",
    "# get percentage of car shares affected depending threshold (connect cars)\n",
    "mask = (data[\"state\"] == \"ended\") & (data[\"previous_id\"].notnull())\n",
    "for i in range(0,len(thresholds)):\n",
    "    data_plot112.loc[i,\"threshold\"] = thresholds[i]\n",
    "    data_plot112.loc[i,\"type\"] = \"connect cars\"\n",
    "    data_plot112.loc[i,\"percent_total\"] = data.loc[mask & (data[\"checkin_type\"] == \"connect\") & \\\n",
    "        (data[\"time_delta\"] < thresholds[i]),:].shape[0] / data.shape[0] * 100\n",
    "    \n",
    "# compile data\n",
    "data_plot11 = pd.concat([data_plot111,data_plot112], axis = 0)\n",
    "\n",
    "# rename and save\n",
    "data_plot11.to_csv(\"streamlit/cnm_bloc5_data_plot11.csv\", index = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - Prediction of pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - univariate analysis ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig1 = make_subplots(\n",
    "    rows = 4,\n",
    "    cols = 4,\n",
    "    subplot_titles = (\n",
    "        \"A. Rental price\", \"B. Mileage\", \"C. Engine power\", \"\",\n",
    "        \"D. Model key\", \"E. Paint color\", \"F. Car type\", \"G. Fuel\",\n",
    "        \"G. Private parking\", \"H. Gps\", \"I. Air conditioning\", \"J. Automatic\",\n",
    "        \"G. Connect\", \"H. Speed regulator\", \"I. Winter tires\"),\n",
    "    vertical_spacing = 0.12,\n",
    "    horizontal_spacing = 0.05)\n",
    "\n",
    "# plot distribution of each numeric variable\n",
    "features_num = [\"rental_price_per_day\",\"mileage\", \"engine_power\"]\n",
    "[fig1.add_trace(go.Histogram(\n",
    "    x = data_ml[features_num[i]],\n",
    "    marker_color = px.colors.qualitative.Vivid[i]),\n",
    "    row = 1, col = i+1) for i in [0, 1, 2]]\n",
    "\n",
    "# plot categorical variables\n",
    "features_cat = [\"model_key\",\"paint_color\",\"car_type\",\"fuel\"]\n",
    "for i in [0, 1, 2, 3]:\n",
    "    data_current = data_ml[features_cat[i]].value_counts()\n",
    "    fig1.add_trace(go.Bar(\n",
    "        x = data_current.index,\n",
    "        y = data_current.values,\n",
    "        marker_color = px.colors.qualitative.Vivid[3:]),\n",
    "        row = 2, col = i+1)\n",
    "features_cat = [\"private_parking_available\",\"has_gps\",\"has_air_conditioning\",\"automatic_car\"]\n",
    "for i in [0, 1, 2, 3]:\n",
    "    data_current = data_ml[features_cat[i]].value_counts()\n",
    "    fig1.add_trace(go.Bar(\n",
    "        x = data_current.index,\n",
    "        y = data_current.values,\n",
    "        marker_color = px.colors.qualitative.Vivid[2:]),\n",
    "        row = 3, col = i+1)\n",
    "features_cat = [\"has_getaround_connect\",\"has_speed_regulator\",\"winter_tires\"]\n",
    "for i in [0, 1, 2]:\n",
    "    data_current = data_ml[features_cat[i]].value_counts()\n",
    "    fig1.add_trace(go.Bar(\n",
    "        x = data_current.index,\n",
    "        y = data_current.values,\n",
    "        marker_color = px.colors.qualitative.Vivid[2:]),\n",
    "        row = 4, col = i+1)\n",
    "\n",
    "# update layout\n",
    "fig1.update_xaxes(tickfont = dict(size = 8))\n",
    "fig1.update_yaxes(tickfont = dict(size = 8))\n",
    "fig1.update_layout(\n",
    "        margin = dict(l = 60, r= 50, t= 140),\n",
    "        title_text = \"Figure 1. Univariate analysis\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        bargroupgap = 0.2,\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 1000)\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - correlation matrix ### ----\n",
    "\n",
    "# get correlation matrix\n",
    "features_num = [\"rental_price_per_day\",\"mileage\", \"engine_power\"]\n",
    "corr_matrix = data_ml.loc[:,features_num].corr().round(2)\n",
    "\n",
    "# plot correlation matrix\n",
    "fig2 = ff.create_annotated_heatmap(corr_matrix.values,\n",
    "                                  x = corr_matrix.columns.tolist(),\n",
    "                                  y = corr_matrix.index.tolist())\n",
    "\n",
    "# update layout\n",
    "fig2.update_layout(\n",
    "        margin = dict(l = 180, b = 40, t = 80),\n",
    "        title_text = \"Figure 2. Correlation matrix\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,   \n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - preprocessing ### ----\n",
    "\n",
    "# drop outliers in mileage\n",
    "upper_bound = data_ml[\"mileage\"].mean() + 3 * data_ml[\"mileage\"].std()\n",
    "data_ml = data_ml.loc[data_ml[\"mileage\"] < upper_bound,:]\n",
    "\n",
    "# drop outliers in engine power\n",
    "lower_bound = data_ml[\"engine_power\"].mean() - 3 * data_ml[\"engine_power\"].std()\n",
    "upper_bound = data_ml[\"engine_power\"].mean() + 3 * data_ml[\"engine_power\"].std()\n",
    "data_ml = data_ml.loc[(data_ml[\"engine_power\"] < upper_bound) & (data_ml[\"engine_power\"] > lower_bound),:]\n",
    "\n",
    "# drop some useless columns\n",
    "data_ml = data_ml.drop([\"Unnamed: 0\"], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - preprocessing for machine learning ### ----\n",
    "\n",
    "# separate target variable Y from features X\n",
    "X = data_ml.drop([\"rental_price_per_day\"], axis = 1)\n",
    "Y = data_ml[\"rental_price_per_day\"]\n",
    "\n",
    "# divide dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# create preprocessor object from pipelines for numeric and categorical features\n",
    "features_num = X._get_numeric_data().columns\n",
    "features_cat = X.drop(features_num, axis = 1).columns\n",
    "numeric_transformer = Pipeline(steps = [\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    (\"encoder\", OneHotEncoder(drop = \"first\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    (\"num\", numeric_transformer, features_num),\n",
    "    (\"cat\", categorical_transformer, features_cat)\n",
    "])\n",
    "\n",
    "# scale numeric features, encode categorical features\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - train model and assess performance ### ----\n",
    "\n",
    "# train model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on train and test sets\n",
    "Y_train_pred = regressor.predict(X_train)\n",
    "Y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# assess performance and print report\n",
    "r2_train = r2_score(Y_train, Y_train_pred)\n",
    "r2_test = r2_score(Y_test, Y_test_pred)\n",
    "print(\"R2 score on training set: \", r2_train)\n",
    "print(\"R2 score on test set: \", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - save transformer and model ### ----\n",
    "\n",
    "# save transformer\n",
    "file_name = \"fastapi/preprocessor.pkl\"\n",
    "pickle.dump(preprocessor, open(file_name, \"wb\"))\n",
    "\n",
    "# save model\n",
    "file_name = \"fastapi/model.pkl\"\n",
    "pickle.dump(regressor, open(file_name, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - prediction of pricing - api testing ### ----\n",
    "\n",
    "# get example data from the dataset\n",
    "datatest = data_ml.iloc[0,:-1].to_dict()\n",
    "\n",
    "# post request\n",
    "response = requests.post(\"https://cnmgetaroundprediction.herokuapp.com/predict\", json = datatest)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
